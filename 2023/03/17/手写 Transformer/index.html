<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>手写 Transformer | SPFA 的博客</title><meta name="keywords" content="python,机器学习"><meta name="author" content="SPFA"><meta name="copyright" content="SPFA"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="纯手工打造的 Transformer 模型">
<meta property="og:type" content="article">
<meta property="og:title" content="手写 Transformer">
<meta property="og:url" content="https://sp-fa.github.io/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/index.html">
<meta property="og:site_name" content="SPFA 的博客">
<meta property="og:description" content="纯手工打造的 Transformer 模型">
<meta property="og:locale">
<meta property="og:image" content="https://sp-fa.github.io/img/cover/lty-bg14.png">
<meta property="article:published_time" content="2023-03-17T15:05:30.000Z">
<meta property="article:modified_time" content="2023-03-17T15:21:47.771Z">
<meta property="article:author" content="SPFA">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sp-fa.github.io/img/cover/lty-bg14.png"><link rel="shortcut icon" href="/img/Tianyi.png"><link rel="canonical" href="https://sp-fa.github.io/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '手写 Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-17 23:21:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/Tianyi.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover/lty-bg14.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">SPFA 的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">手写 Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-03-17T15:05:30.000Z" title="Created 2023-03-17 23:05:30">2023-03-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-03-17T15:21:47.771Z" title="Updated 2023-03-17 23:21:47">2023-03-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">1.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>8min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="手写 Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>先放出 transformer 的整体结构图，以便复习，接下来就一个模块一个模块的实现它。<br><img src="Transformer.png" alt="在这里插入图片描述"></p>
<hr>
<h1 id="1-Embedding"><a href="#1-Embedding" class="headerlink" title="1. Embedding"></a>1. Embedding</h1><p><img src="Embedding.png" alt="在这里插入图片描述"></p>
<p>Embedding 部分主要由两部分组成，即 Input Embedding 和 Positional Encoding，位置编码记录了每一个词出现的位置。通过加入位置编码可以提高模型的准确率，因为同一个词出现在不同位置可能代表了不同意思，这直接影响了最终的结果，所以要考虑位置因素。</p>
<p>位置编码公式：</p>
<script type="math/tex; mode=display">PE(pos, 2i)=\sin(\frac{pos}{10000^\frac{2i}{d}})</script><script type="math/tex; mode=display">PE(pos,2i+1)=\cos(\frac{pos}{10000^\frac{2i}{d}})</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_angles</span>(<span class="params">pos, i, d</span>):</span></span><br><span class="line">    <span class="keyword">return</span>  pos / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positional_encoding</span>(<span class="params">position, d</span>):</span></span><br><span class="line">    theta = get_angles(np.arange(position)[:, np.newaxis], np.arange(d)[np.newaxis, :], d)</span><br><span class="line"></span><br><span class="line">    theta[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(theta[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    theta[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(theta[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta[np.newaxis, ...] <span class="comment"># shape: [1, position, d]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embedding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embedding, self).__init__()</span><br><span class="line">        self.dim = cfg.hidden_dim</span><br><span class="line">        self.device = cfg.device</span><br><span class="line">        self.word_em = nn.Embedding(num_embeddings=cfg.vocab_size, embedding_dim=self.dim).to(self.device)</span><br><span class="line">        self.position_em = positional_encoding(cfg.max_len, self.dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids</span>):</span></span><br><span class="line">        seq_length = input_ids.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        we = self.word_em(input_ids)</span><br><span class="line">        we *= torch.sqrt(self.dim).to(self.device)</span><br><span class="line">        pe = self.position_em[:, :seq_length, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> we + pe</span><br></pre></td></tr></table></figure>
<p>其中 cfg 文件用来存一些超参数，device 是选择使用 CPU / GPU 的设备编号。</p>
<hr>
<h1 id="2-Masking"><a href="#2-Masking" class="headerlink" title="2. Masking"></a>2. Masking</h1><p>为一些标记添加遮罩，这里主要用于一下两种情况：</p>
<ol>
<li>对填充标记 <code>[PAD]</code> 进行遮罩，确保模型不会将填充作为输入。</li>
<li>前瞻遮挡（look-ahead mask）用于遮挡一个序列中的后续部分，比如说要预测第三个词，那么前面的 <code>[CLS]</code> 标记以及第一第二个词被保留用于预测，而其它位置的词则应该被遮住，不能输入模型。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_padding_mask</span>(<span class="params">seq</span>):</span></span><br><span class="line">    seq.eq_(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 添加额外的维度来将填充加到注意力对数（logits）。</span></span><br><span class="line">    <span class="keyword">return</span> seq[:, np.newaxis, np.newaxis, :]  <span class="comment"># shape: [batch_size, 1, 1, seq_len]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_look_ahead_mask</span>(<span class="params">seq_len</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.triu(torch.ones(seq_len, seq_len), <span class="number">1</span>)  <span class="comment"># shape: [seq_len, seq_len]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masks</span>(<span class="params">src_ids, trg_ids</span>):</span></span><br><span class="line">    en_padding_mask = create_padding_mask(src_ids)</span><br><span class="line">    de_padding_mask = create_padding_mask(src_ids)</span><br><span class="line"></span><br><span class="line">    look_ahead_mask = create_look_ahead_mask(trg_ids.shape[<span class="number">1</span>])</span><br><span class="line">    de_trg_padding_mask = create_padding_mask(trg_ids)</span><br><span class="line">    combined_mask = torch.maximum(de_trg_padding_mask, look_ahead_mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> en_padding_mask, combined_mask, de_padding_mask</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="3-Attention"><a href="#3-Attention" class="headerlink" title="3. Attention"></a>3. Attention</h1><h2 id="3-1-Scaled-Dot-Product-Attention-按比缩放的点积注意力"><a href="#3-1-Scaled-Dot-Product-Attention-按比缩放的点积注意力" class="headerlink" title="3.1 Scaled Dot-Product Attention (按比缩放的点积注意力)"></a>3.1 Scaled Dot-Product Attention (按比缩放的点积注意力)</h2><p><img src="Attention.png" alt="在这里插入图片描述"><br>点积注意力机制的公式为：</p>
<script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d}})V</script><p>点积注意力被缩小了深度的平方根倍。这样做是因为对于较大的深度值，点积的大小会增大，从而推动 Softmax 函数往仅有很小的梯度的方向靠拢，这样可能会导致梯度消失。</p>
<p>例如，假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 dk。因此，dk 的平方根被用于缩放（而非其他数值），因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样会获得一个更平缓的 Softmax。</p>
<p>在 Softmax 之前要记得加入遮罩，遮罩乘以 -1e9 来添加一个无穷小的数，使得 Softmax 之后的输出在被遮住的部分接近于 0，这样就可以忽略该位置的信息。</p>
<h2 id="3-2-Multi-Head-Attention"><a href="#3-2-Multi-Head-Attention" class="headerlink" title="3.2 Multi-Head Attention"></a>3.2 Multi-Head Attention</h2><p><img src="MultiHead.png" alt="在这里插入图片描述"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.n_head = cfg.n_head</span><br><span class="line">        self.dim = cfg.hidden_dim</span><br><span class="line">        self.device = cfg.device</span><br><span class="line"></span><br><span class="line">        self.wq = nn.Linear(self.dim, self.dim).to(self.device)</span><br><span class="line">        self.wk = nn.Linear(self.dim, self.dim).to(self.device)</span><br><span class="line">        self.wv = nn.Linear(self.dim, self.dim).to(self.device)</span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">3</span>)</span><br><span class="line">        self.f = nn.Linear(self.dim, self.dim).to(cfg.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split</span>(<span class="params">self, tensor</span>):</span></span><br><span class="line">        a, b, c = tensor.size()</span><br><span class="line">        d = c // self.n_head</span><br><span class="line">        <span class="keyword">return</span> tensor.view(a, b, self.n_head, d).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">concat</span>(<span class="params">self, tensor</span>):</span></span><br><span class="line">        a, b, c, d = tensor.size()</span><br><span class="line">        <span class="keyword">return</span> tensor.view(a, c, b * d)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">self, k, q, v, mask</span>):</span></span><br><span class="line">        _, _, _, d = k.size()</span><br><span class="line">        kt = torch.transpose(k, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        s = (q @ kt) / math.sqrt(d) <span class="comment"># Scale 操作，防止之后 Softmax 时梯度消失</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            s += (mask * -<span class="number">1e9</span>);</span><br><span class="line"></span><br><span class="line">        s = self.softmax(s)</span><br><span class="line">        v = s @ v</span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, v, k, q, mask</span>):</span></span><br><span class="line">        k, q, v = self.wk(v), self.wq(k), self.wv(q)</span><br><span class="line">        k, q, v = self.split(k), self.split(q), self.split(v)</span><br><span class="line">        output = self.attention(k, q, v, mask)</span><br><span class="line">        output = self.concat(output)</span><br><span class="line">        <span class="keyword">return</span> self.f(output)</span><br></pre></td></tr></table></figure></p>
<hr>
<h1 id="4-Add-amp-Norm"><a href="#4-Add-amp-Norm" class="headerlink" title="4. Add &amp; Norm"></a>4. Add &amp; Norm</h1><p><img src="AddNorm.png" alt="在这里插入图片描述"></p>
<p>残差连接层有助于避免深度网络中的梯度消失问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__()</span><br><span class="line">        self.dim = cfg.hidden_dim</span><br><span class="line"></span><br><span class="line">        self.norm = nn.LayerNorm(self.dim).to(cfg.device)</span><br><span class="line">        self.dropout = nn.Dropout(cfg.drop_out) <span class="comment"># 正则化，参数为正则化概率</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, x</span>):</span></span><br><span class="line">        <span class="built_in">input</span> = self.dropout(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> self.norm(<span class="built_in">input</span> + x)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="5-Point-wise-feed-forward-network-点式前馈网络"><a href="#5-Point-wise-feed-forward-network-点式前馈网络" class="headerlink" title="5. Point wise feed forward network (点式前馈网络)"></a>5. Point wise feed forward network (点式前馈网络)</h1><p><img src="FeedForward.png" alt="在这里插入图片描述"><br>点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FeedForward, self).__init__()</span><br><span class="line">        self.dim = cfg.hidden_dim</span><br><span class="line"></span><br><span class="line">        self.f1 = nn.Linear(self.dim, self.dim).to(cfg.device)</span><br><span class="line">        self.relu = nn.ReLU().to(cfg.device)</span><br><span class="line">        self.f2 = nn.Linear(self.dim, self.dim).to(cfg.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.f1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> self.f2(x)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="6-Encoder-and-Decoder"><a href="#6-Encoder-and-Decoder" class="headerlink" title="6. Encoder and Decoder"></a>6. Encoder and Decoder</h1><h2 id="6-1-Encoder-Layer"><a href="#6-1-Encoder-Layer" class="headerlink" title="6.1 Encoder Layer"></a>6.1 Encoder Layer</h2><p><img src="Encoder.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.attention = MultiHeadAttention(cfg)</span><br><span class="line">        self.fforward = FeedForward(cfg)</span><br><span class="line"></span><br><span class="line">        self.addnorm1 = AddNorm(cfg)</span><br><span class="line">        self.addnorm2 = AddNorm(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        attn = self.attention(x, x, x, mask)</span><br><span class="line">        attn = self.addnorm1(attn, x)</span><br><span class="line"></span><br><span class="line">        ffw = self.fforward(attn)</span><br><span class="line">        ffw = self.addnorm2(ffw, attn)</span><br><span class="line">        <span class="keyword">return</span> ffw</span><br></pre></td></tr></table></figure>
<h2 id="6-2-Encoder"><a href="#6-2-Encoder" class="headerlink" title="6.2 Encoder"></a>6.2 Encoder</h2><p>编码器包括</p>
<ol>
<li>Embedding</li>
<li>N 个编码器层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.num_layers = cfg.encoder_layer_count</span><br><span class="line"></span><br><span class="line">        self.embedding = Embedding(cfg)</span><br><span class="line">        self.layers = [EncoderLayer(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers)]</span><br><span class="line">        self.dropout = nn.Dropout(cfg.drop_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            x = self.layers[i](x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="6-3-Decoder-Layer"><a href="#6-3-Decoder-Layer" class="headerlink" title="6.3 Decoder Layer"></a>6.3 Decoder Layer</h2><p><img src="Decoder.png" alt="在这里插入图片描述"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.attention1 = MultiHeadAttention(cfg)</span><br><span class="line">        self.attention2 = MultiHeadAttention(cfg)</span><br><span class="line"></span><br><span class="line">        self.fforward = FeedForward(cfg)</span><br><span class="line"></span><br><span class="line">        self.addnorm1 = AddNorm(cfg)</span><br><span class="line">        self.addnorm2 = AddNorm(cfg)</span><br><span class="line">        self.addnorm3 = AddNorm(cfg)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, encoder_out, look_ahead_mask, padding_mask</span>):</span></span><br><span class="line">        attn1 = self.attention1(x, x, x, look_ahead_mask)</span><br><span class="line">        ffw1 = self.addnorm1(attn1, x)</span><br><span class="line"></span><br><span class="line">        attn2 = self.attention2(encoder_out, encoder_out, ffw1, padding_mask)</span><br><span class="line">        ffw2 = self.addnorm2(attn2, ffw1)</span><br><span class="line"></span><br><span class="line">        output = self.fforward(ffw2)</span><br><span class="line">        output = self.addnorm3(output, ffw2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h2 id="6-4-Decoder"><a href="#6-4-Decoder" class="headerlink" title="6.4 Decoder"></a>6.4 Decoder</h2><p>解码器包括</p>
<ol>
<li>Embedding</li>
<li>N 个解码器层</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.dim = cfg.dim</span><br><span class="line">        self.num_layers = cfg.decoder_layer_count</span><br><span class="line"></span><br><span class="line">        self.embedding = Embedding(cfg)</span><br><span class="line">        self.layers = [Decoder(cfg) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers)]</span><br><span class="line">        self.dropout = nn.Dropout(cfg.drop_out)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, encode_output, look_ahead_mask, padding_mask</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            x = self.layers[i](x, encode_output, look_ahead_mask, padding_mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="7-Transformer"><a href="#7-Transformer" class="headerlink" title="7. Transformer"></a>7. Transformer</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, cfg</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder(cfg)</span><br><span class="line">        self.decoder = Decoder(cfg)</span><br><span class="line">        self.linear = nn.Linear(cfg.hidden_dim, cfg.vocab_size).to(cfg.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src_ids, trg_ids, en_padding_mask, look_ahead_mask, de_padding_mask</span>):</span></span><br><span class="line">        en_output = self.encoder(src_ids, en_padding_mask)</span><br><span class="line">        de_output = self.decoder(trg_ids, en_output, look_ahead_mask, de_padding_mask)</span><br><span class="line">        <span class="keyword">return</span> self.linear(de_output)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="8-优化器-与-损失函数"><a href="#8-优化器-与-损失函数" class="headerlink" title="8. 优化器 与 损失函数"></a>8. 优化器 与 损失函数</h1><p>根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。</p>
<script type="math/tex; mode=display">lrate=d^{-\frac12}*\min(num^{-\frac12},num*warmup\_steps^{-\frac32})</script><p>warmup_steps 为 4000 时的学习率变化如图：</p>
<p><img src="Schedule.png" alt="在这里插入图片描述"><br>由于目标序列是 padding 过的，因此在计算损失函数时，应该使用 mask 遮去 <code>[PAD]</code> 的部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">y, pred</span>):</span></span><br><span class="line">    mask = torch.logical_not(torch.eq(y, <span class="number">0</span>))</span><br><span class="line">    loss = criterion(real, pred) <span class="comment"># 损失函数</span></span><br><span class="line">    </span><br><span class="line">    mask = mask.<span class="built_in">float</span>()</span><br><span class="line">    loss *= mask</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.mean(loss)</span><br></pre></td></tr></table></figure>
<hr>
<h1 id="9-Tokenizer"><a href="#9-Tokenizer" class="headerlink" title="9. Tokenizer"></a>9. Tokenizer</h1><p>顺便学一下训练一个自己的 tokenizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>)) <span class="comment"># 用 [UNK] 替换未知字符</span></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>( <span class="comment"># 按一定规则标准化：NFD：统一 Unicode 编码</span></span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase()]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit() <span class="comment"># 使用 空格 来分割</span></span><br><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">10000</span>, special_tokens=special_tokens) <span class="comment"># 分成 10000 类</span></span><br><span class="line"></span><br><span class="line">tokenizer.train([<span class="string">&#x27;text.txt&#x27;</span>], trainer=trainer) <span class="comment"># 用于训练的文本</span></span><br><span class="line">tokenizer.save(<span class="string">&#x27;my_tokenizer.json&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">tokenizer = Tokenizer.from_file(<span class="string">&quot;my_tokenizer.json&quot;</span>) <span class="comment"># 调用自己的 tokenizer</span></span><br><span class="line"></span><br><span class="line">s = <span class="string">&quot;给他们来点小小的蜀国震撼&quot;</span></span><br><span class="line">tokens = tokenizer.encode(s)</span><br><span class="line"><span class="built_in">print</span>(tokens.ids, tokens.tokens)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://SP-FA.github.io">SPFA</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://sp-fa.github.io/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/">https://sp-fa.github.io/2023/03/17/手写 Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/cover/lty-bg14.png" data-sites="facebook,twitter,wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%A4%A7%E5%85%A8/"><img class="next-cover" src="/img/cover/lty-bg15.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">机器学习 10：激活函数大全</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/07/28/keras.layers.Conv2D()-%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0/" title="keras.layers.Conv2D() 函数参数"><img class="cover" src="/img/cover/rinlen-bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-28</div><div class="title">keras.layers.Conv2D() 函数参数</div></div></a></div><div><a href="/2021/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-01%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="机器学习 01：感知机"><img class="cover" src="/img/cover/lty-bg19.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-10</div><div class="title">机器学习 01：感知机</div></div></a></div><div><a href="/2021/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-02%EF%BC%9A%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F/" title="机器学习 02：感知机的对偶形式"><img class="cover" src="/img/cover/lty-bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-10</div><div class="title">机器学习 02：感知机的对偶形式</div></div></a></div><div><a href="/2021/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-03%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="机器学习 03：线性可分支持向量机"><img class="cover" src="/img/cover/rinlen-bg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-12</div><div class="title">机器学习 03：线性可分支持向量机</div></div></a></div><div><a href="/2021/07/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-04%EF%BC%9A%E8%BD%AF%E9%97%B4%E9%9A%94%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="机器学习 04：软间隔支持向量机"><img class="cover" src="/img/cover/hutao-bg1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-14</div><div class="title">机器学习 04：软间隔支持向量机</div></div></a></div><div><a href="/2021/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-05%EF%BC%9A%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="机器学习 05：非线性支持向量机"><img class="cover" src="/img/cover/hutao-bg3.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-21</div><div class="title">机器学习 05：非线性支持向量机</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/Tianyi.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">SPFA</div><div class="author-info__description">我永远喜欢洛天依</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/SP-FA"><i class="fab fa-github"></i><span>关注我！</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/sp-fa" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2997839760@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">博主打 apex 去辣！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Embedding"><span class="toc-number">1.</span> <span class="toc-text">1. Embedding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Masking"><span class="toc-number">2.</span> <span class="toc-text">2. Masking</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Attention"><span class="toc-number">3.</span> <span class="toc-text">3. Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Scaled-Dot-Product-Attention-%E6%8C%89%E6%AF%94%E7%BC%A9%E6%94%BE%E7%9A%84%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 Scaled Dot-Product Attention (按比缩放的点积注意力)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Multi-Head-Attention"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Multi-Head Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Add-amp-Norm"><span class="toc-number">4.</span> <span class="toc-text">4. Add &amp; Norm</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Point-wise-feed-forward-network-%E7%82%B9%E5%BC%8F%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">5. Point wise feed forward network (点式前馈网络)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Encoder-and-Decoder"><span class="toc-number">6.</span> <span class="toc-text">6. Encoder and Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-Encoder-Layer"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 Encoder Layer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Encoder"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 Encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Decoder-Layer"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 Decoder Layer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-Decoder"><span class="toc-number">6.4.</span> <span class="toc-text">6.4 Decoder</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Transformer"><span class="toc-number">7.</span> <span class="toc-text">7. Transformer</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E4%BC%98%E5%8C%96%E5%99%A8-%E4%B8%8E-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">8. 优化器 与 损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-Tokenizer"><span class="toc-number">9.</span> <span class="toc-text">9. Tokenizer</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/" title="手写 Transformer"><img src="/img/cover/lty-bg14.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="手写 Transformer"/></a><div class="content"><a class="title" href="/2023/03/17/%E6%89%8B%E5%86%99%20Transformer/" title="手写 Transformer">手写 Transformer</a><time datetime="2023-03-17T15:05:30.000Z" title="Created 2023-03-17 23:05:30">2023-03-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%A4%A7%E5%85%A8/" title="机器学习 10：激活函数大全"><img src="/img/cover/lty-bg15.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习 10：激活函数大全"/></a><div class="content"><a class="title" href="/2023/01/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-10%EF%BC%9A%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%A4%A7%E5%85%A8/" title="机器学习 10：激活函数大全">机器学习 10：激活函数大全</a><time datetime="2023-01-12T16:51:53.000Z" title="Created 2023-01-13 00:51:53">2023-01-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-09%EF%BC%9AGram%E7%9F%A9%E9%98%B5/" title="机器学习 09：Gram 矩阵"><img src="/img/cover/lty-bg7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习 09：Gram 矩阵"/></a><div class="content"><a class="title" href="/2022/06/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-09%EF%BC%9AGram%E7%9F%A9%E9%98%B5/" title="机器学习 09：Gram 矩阵">机器学习 09：Gram 矩阵</a><time datetime="2022-06-12T08:56:45.000Z" title="Created 2022-06-12 16:56:45">2022-06-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-08%EF%BC%9APCA/" title="机器学习 08：PCA"><img src="/img/cover/lty-bg19.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习 08：PCA"/></a><div class="content"><a class="title" href="/2022/05/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-08%EF%BC%9APCA/" title="机器学习 08：PCA">机器学习 08：PCA</a><time datetime="2022-05-28T07:28:40.000Z" title="Created 2022-05-28 15:28:40">2022-05-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-07%EF%BC%9A%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3%E5%92%8C%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/" title="机器学习 07：特征分解和奇异值分解"><img src="/img/cover/mocha-bg3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习 07：特征分解和奇异值分解"/></a><div class="content"><a class="title" href="/2022/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-07%EF%BC%9A%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3%E5%92%8C%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/" title="机器学习 07：特征分解和奇异值分解">机器学习 07：特征分解和奇异值分解</a><time datetime="2022-03-15T12:53:43.000Z" title="Created 2022-03-15 20:53:43">2022-03-15</time></div></div></div></div></div></div></main><footer id="footer" style="background: #FFFFFF"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By SPFA</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">祝你每天有个好心情~</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '0141efa3e0ee57e12694',
      clientSecret: 'f3a584771d3758ca19a12f72e30e9c34e33cbdf3',
      repo: 'SP-FA.github.io',
      owner: 'SPFA',
      admin: ['SPFA'],
      id: 'cdb994c8da8120d19634e87c708c2609',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><div class="aplayer no-destroy" data-id="4989873047" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-volume=0.2> </div><script src="/js/rphoto.js"></script><script data-pjax>obcbo()</script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer@1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>